{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f339f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Force unsloth to use the local GPU memory efficiently\n",
    "os.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ec983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m!pip install uv\n",
    "# Cell 1: Installs\n",
    "# We need 'trl' for GRPO and 'unsloth' for the model\n",
    "!uv pip install unsloth vllm\n",
    "!uv pip install --no-deps trl peft accelerate bitsandbytes\n",
    "!uv pip uninstall xformers\n",
    "!uv pip install -q datasets\n",
    "!uv pip install -q pandas\n",
    "!uv pip install -q tensorboard\n",
    "!uv pip install -q -U \"huggingface-hub>=0.34.0,<1.0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61690efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv venv\n",
    "# !uv pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6daa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install torch torchvision\n",
    "# !uv pip install \"transformers>=5.0.0rc1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437aea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "try:\n",
    "    # Fix for xformers/triton incompatibility\n",
    "    sys.modules['xformers'] = None\n",
    "except: pass\n",
    "\n",
    "# Cell 2: Load Model (Gemma 3 4B)\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096 # Reasoning needs space!\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\", # Or \"unsloth/gemma-3-4b-it\" if 4bit not up yet\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# Enable PEFT (LoRA)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Critical for 12GB VRAM\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: The HICRA Logic (Strategic Grams)\n",
    "\n",
    "# These are the \"thinking words\" the paper identified. \n",
    "# When the model uses these, it is \"planning\".\n",
    "STRATEGIC_GRAMS = [\n",
    "    \"first i need to\", \"let's look at\", \"alternatively\", \"wait\", \n",
    "    \"but i'm not sure\", \"let's see if\", \"notice that\", \n",
    "    \"the final answer is\", \"let's assume\", \"we can conclude\",\n",
    "    \"implies that\", \"to solve this\", \"break it down\", \n",
    "    \"suppose that\", \"checking the\", \"recall that\"\n",
    "]\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward = 1.0 if the final answer is correct, 0.0 otherwise.\n",
    "    This is the \"Ground Truth\" signal.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion, correct_ans in zip(completions, answer):\n",
    "        # Simple check: is the answer roughly in the text?\n",
    "        # In a real system, you'd extract the number exactly.\n",
    "        # For now, we check if the correct string appears in the output.\n",
    "        if str(correct_ans) in completion:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "def hicra_planning_reward_func(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    HICRA Proxy: Gives a small bonus for using 'Strategic Grams'.\n",
    "    This encourages the model to 'think' before answering.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        score = 0.0\n",
    "        # Check for presence of planning words\n",
    "        completion_lower = completion.lower()\n",
    "        for gram in STRATEGIC_GRAMS:\n",
    "            if gram in completion_lower:\n",
    "                score += 0.1 # Small bonus for EACH planning step\n",
    "        \n",
    "        # Cap the bonus so it doesn't game the system just by spamming words\n",
    "        rewards.append(min(score, 0.5)) \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb281a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Prepare Data for GRPO\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the file you generated with the API script\n",
    "dataset = load_dataset(\"json\", data_files=\"reasoning_dataset.json\", split=\"train\")\n",
    "\n",
    "# GRPO expects a specific format. We don't need a system prompt for simple math.\n",
    "# It just needs 'prompt' and 'answer' (which we generated).\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Train with GRPO\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "# Load the TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard pointing to your output directory\n",
    "# (Make sure 'gemma-3-reasoning-output' matches the 'output_dir' in your GRPOConfig!)\n",
    "%tensorboard --logdir gemma-3-reasoning-output\n",
    "\n",
    "# Configuration for 12GB VRAM\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"gemma-3-reasoning-output\",\n",
    "    learning_rate=2e-5, # RL usually needs lower LR\n",
    "    per_device_train_batch_size=1, # Keep small for VRAM\n",
    "    gradient_accumulation_steps=8, \n",
    "    max_prompt_length=512,\n",
    "    max_completion_length=512, # Allow it to think!\n",
    "    num_generations=2, # Generate 2 answers per question to compare\n",
    "    max_steps=200, # Quick run to test\n",
    "    use_vllm=False,\n",
    "    save_steps=50,\n",
    "    logging_steps=1,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[correctness_reward_func, hicra_planning_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start the RL Loop!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402268d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from step 60 to step 180\n",
    "trainer.args.max_steps = 180  # New target\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from step 60 to step 180\n",
    "trainer.args.max_steps = 270  # New target\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab47cb9",
   "metadata": {},
   "source": [
    "Set up the transformers inference API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b086751",
   "metadata": {},
   "source": [
    "1. Adjusting Your Script for the Project\n",
    "Here is the adjusted script. I have updated it to fit the Gemma-9B context and added a safety step to clear memory before merging (crucial on cloud GPUs to avoid crashing at the finish line).\n",
    "\n",
    "You should append this to the end of your training notebook/script.\n",
    "\n",
    "2. Important Step for HF Spaces\n",
    "You must add your Hugging Face Token as a Secret in the Space settings, or the script won't be able to push the model.\n",
    "\n",
    "Go to your Space -> Settings.\n",
    "\n",
    "Scroll to \"Variables and secrets\".\n",
    "\n",
    "Add a New Secret: HF_TOKEN -> [Paste your Write token]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- 1. MEMORY CLEANUP (Crucial for Cloud) ---\n",
    "# RL Training fills VRAM. We need to clear it before the heavy \"Merge\" step.\n",
    "print(\"üßπ Cleaning up VRAM before merging...\")\n",
    "try:\n",
    "    del trainer\n",
    "    del batch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# --- 2. RELOAD MODEL FOR MERGING ---\n",
    "# Sometimes it's safer to reload the base model + adapter freshly to merge\n",
    "# independent of the messy training state.\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"üîÑ Reloading model for clean merge...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-9b-it-bnb-4bit\", # Your base model\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Load the adapters you just trained\n",
    "# Assuming your GRPOConfig output_dir was \"gemma-reasoning-output\"\n",
    "# and the latest checkpoint is saved there.\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"gemma-reasoning-output/checkpoint-final\") # Update path to your actual checkpoint folder!\n",
    "\n",
    "# --- 3. LOGIN & PUSH ---\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No HF_TOKEN found! Check your Space 'Settings' -> 'Variables' to add it.\")\n",
    "\n",
    "repo_name = \"david-barnes/Gemma-2-9B-Reasoning-v1\" # Your new repo name\n",
    "\n",
    "print(f\"‚è≥ Merging to 16-bit and Pushing to: {repo_name}...\")\n",
    "\n",
    "# This takes care of the de-quantization and merging in one go\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\", # 16-bit is best for sharing reasoning models\n",
    "    token = hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Success! Your reasoning model is live.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed046041",
   "metadata": {},
   "source": [
    "### 3. Configure LoRA:\n",
    "\n",
    "Unsloth handles the target modules automatically (including the tricky gate_proj, up_proj, etc. that vanilla Peft requires you to list manually)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde9498",
   "metadata": {},
   "source": [
    "### Check where the model is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098082b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where the model is cached\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/\")\n",
    "print(f\"Model cache location: {cache_dir}\")\n",
    "print(\"\\nContents:\")\n",
    "if os.path.exists(cache_dir):\n",
    "    for item in os.listdir(cache_dir)[:10]:  # Show first 10 items\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"Cache directory not found yet\")\n",
    "\n",
    "# You can also set a custom cache location if you prefer:\n",
    "# os.environ['HF_HOME'] = '/path/to/custom/cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc77db5",
   "metadata": {},
   "source": [
    "## Apply QLora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381ced9",
   "metadata": {},
   "source": [
    "Quick calculation:\n",
    "\n",
    "700 records\n",
    "Effective batch size = per_device_batch_size (2) √ó gradient_accumulation_steps (4) = 8\n",
    "Steps per epoch = 700 / 8 = ~88 steps\n",
    "So 60 steps = ~0.7 epochs - you haven't even completed one full pass through your data yet!\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "Epochs |\tSteps |\tUse Case |\n",
    "1 |\t~90 |\tMinimum - sees all data once |\n",
    "2-3 |\t~180-270|\tSweet spot for fine-tuning |\n",
    "5+ |\t440+ |\tRisk of overfitting |\n",
    "\n",
    "Since your loss was still decreasing at step 60, you probably have room to train more. I'd suggest trying max_steps = 180 (about 2 epochs) for a good balance.\n",
    "\n",
    "Watch for:\n",
    "\n",
    "‚úÖ Good sign: Loss continues decreasing smoothly\n",
    "‚ö†Ô∏è Overfitting warning: Loss drops very low (<0.1) or starts fluctuating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db7b43",
   "metadata": {},
   "source": [
    "### LOGIN TO HUB\n",
    "\n",
    "When we push to HuggingFace Hub, it will merge our local QLoRa adaptor with the base model we used to train, on the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Try to login with token from environment variable\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "\tlogin(token=hf_token)\n",
    "\tprint(\"Logged in with HF_TOKEN environment variable\") \n",
    "else:\n",
    "\t# Skip login for local training - you can still train without pushing to hub\n",
    "\tprint(\"No HF_TOKEN found. Proceeding with local training on local GPU...\")\n",
    "\tprint(\"Note: You won't be able to push models to HuggingFace Hub without authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fb190",
   "metadata": {},
   "source": [
    "# Push Model to hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52519998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "# Point this to the exact folder on your disk\n",
    "checkpoint_path = \"outputs/checkpoint-180\" \n",
    "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\" # Your Hugging Face repo\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# 2. LOAD SPECIFIC CHECKPOINT\n",
    "# Unsloth is smart: if you point it to a folder, it loads the base model \n",
    "# AND applies the adapters from that folder automatically.\n",
    "print(f\"üìÇ Loading checkpoint from {checkpoint_path}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = checkpoint_path, \n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # Keep True for fast loading (Unsloth handles the merge magic)\n",
    ")\n",
    "\n",
    "# 3. MERGE & PUSH\n",
    "# This will de-quantize the base model, merge your checkpoint-180 adapters, \n",
    "# and upload a clean 16-bit model to the Hub.\n",
    "print(f\"üöÄ Merging and pushing to {repo_name}...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
    "    token = hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Done! Your Junior Accountant (Checkpoint 180) is live!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Unsloth)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
