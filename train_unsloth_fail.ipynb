{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f339f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Force unsloth to use the local GPU memory efficiently\n",
    "os.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ec983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m!pip install uv\n",
    "# Cell 1: Installs\n",
    "# We need 'trl' for GRPO and 'unsloth' for the model\n",
    "!uv pip install unsloth vllm\n",
    "!uv pip install --no-deps trl peft accelerate bitsandbytes\n",
    "!uv pip uninstall xformers\n",
    "!uv pip install -q datasets\n",
    "!uv pip install -q pandas\n",
    "!uv pip install -q tensorboard\n",
    "!uv pip install -q -U \"huggingface-hub>=0.34.0,<1.0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61690efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv venv\n",
    "# !uv pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6daa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install torch torchvision\n",
    "# !uv pip install \"transformers>=5.0.0rc1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437aea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n",
      "/home/user/miniconda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-23 00:15:03] INFO compiler.py:646: Standard import failed for UnslothGRPOTrainer: invalid syntax (UnslothGRPOTrainer.py, line 2500). Using tempfile instead!\n",
      "[2025-12-23 00:15:03] INFO compiler.py:652: Standard import failed for UnslothGRPOTrainer: invalid syntax (UnslothGRPOTrainer.py, line 2500). Using spec.loader.exec_module instead!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Direct module loading failed for UnslothGRPOTrainer: invalid syntax (UnslothGRPOTrainer.py, line 2500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSyntaxError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/compiler.py:637\u001b[39m, in \u001b[36mcreate_new_function\u001b[39m\u001b[34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[39m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     new_module, old_path = \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompile_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/compiler.py:632\u001b[39m, in \u001b[36mcreate_new_function.<locals>.import_module\u001b[39m\u001b[34m(compile_folder, name)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# Try standard import\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m new_module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_module, old_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:936\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1074\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1004\u001b[39m, in \u001b[36msource_to_code\u001b[39m\u001b[34m(self, data, path, _optimize)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[31mSyntaxError\u001b[39m: invalid syntax (UnslothGRPOTrainer.py, line 2500)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mSyntaxError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/compiler.py:662\u001b[39m, in \u001b[36mcreate_new_function\u001b[39m\u001b[34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[39m\n\u001b[32m    661\u001b[39m     sys.modules[module_name] = new_module\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     \u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:936\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1074\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1004\u001b[39m, in \u001b[36msource_to_code\u001b[39m\u001b[34m(self, data, path, _optimize)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[31mSyntaxError\u001b[39m: invalid syntax (UnslothGRPOTrainer.py, line 2500)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m: \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Cell 2: Load Model (Gemma 3 4B)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     12\u001b[39m max_seq_length = \u001b[32m4096\u001b[39m \u001b[38;5;66;03m# Reasoning needs space!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/__init__.py:261\u001b[39m\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msave\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[38;5;250m     \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, FastVisionModel, FastTextModel, FastModel\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmistral\u001b[39;00m\u001b[38;5;250m   \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:3036\u001b[39m\n\u001b[32m   3033\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   3035\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PatchFastRL\n\u001b[32m-> \u001b[39m\u001b[32m3036\u001b[39m \u001b[43mPatchFastRL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFastLanguageModel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mFastLlamaModel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/rl.py:1053\u001b[39m, in \u001b[36mPatchFastRL\u001b[39m\u001b[34m(algorithm, FastLanguageModel)\u001b[39m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mPatchFastRL\u001b[39m(algorithm = \u001b[38;5;28;01mNone\u001b[39;00m, FastLanguageModel = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m FastLanguageModel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: PatchRL(FastLanguageModel)\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     \u001b[43mpatch_trl_rl_trainers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(algorithm) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m algorithm.islower():\n\u001b[32m   1055\u001b[39m         PatchRLStatistics(algorithm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/rl.py:1046\u001b[39m, in \u001b[36mpatch_trl_rl_trainers\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1044\u001b[39m all_trainers = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_trainers \u001b[38;5;28;01mif\u001b[39;00m x.islower() \u001b[38;5;129;01mand\u001b[39;00m x.endswith(\u001b[33m\"\u001b[39m\u001b[33m_trainer\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m trainer \u001b[38;5;129;01min\u001b[39;00m all_trainers:\n\u001b[32m-> \u001b[39m\u001b[32m1046\u001b[39m     \u001b[43m_patch_trl_rl_trainers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/rl.py:805\u001b[39m, in \u001b[36m_patch_trl_rl_trainers\u001b[39m\u001b[34m(trainer_file)\u001b[39m\n\u001b[32m    802\u001b[39m RLTrainer_source = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn]\u001b[39m\u001b[33m{\u001b[39m\u001b[33m3,}\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, RLTrainer_source)\n\u001b[32m    804\u001b[39m \u001b[38;5;66;03m# Create new function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m created_module = \u001b[43mcreate_new_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUnsloth\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mRLTrainer_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mRLTrainer_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrl.trainer.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrainer_file\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[38;5;66;03m# Patch Trainer\u001b[39;00m\n\u001b[32m    814\u001b[39m exec(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrl.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRLTrainer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = created_module.Unsloth\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRLTrainer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/compiler.py:664\u001b[39m, in \u001b[36mcreate_new_function\u001b[39m\u001b[34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[39m\n\u001b[32m    662\u001b[39m             spec.loader.exec_module(new_module)\n\u001b[32m    663\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m664\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirect module loading failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    665\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    667\u001b[39m     \u001b[38;5;66;03m# Restore original sys.path if we modified it\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Direct module loading failed for UnslothGRPOTrainer: invalid syntax (UnslothGRPOTrainer.py, line 2500)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "try:\n",
    "    # Fix for xformers/triton incompatibility\n",
    "    sys.modules['xformers'] = None\n",
    "except: pass\n",
    "\n",
    "# Cell 2: Load Model (Gemma 3 4B)\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096 # Reasoning needs space!\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\", # Or \"unsloth/gemma-3-4b-it\" if 4bit not up yet\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# Enable PEFT (LoRA)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Critical for 12GB VRAM\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: The HICRA Logic (Strategic Grams)\n",
    "\n",
    "# These are the \"thinking words\" the paper identified. \n",
    "# When the model uses these, it is \"planning\".\n",
    "STRATEGIC_GRAMS = [\n",
    "    \"first i need to\", \"let's look at\", \"alternatively\", \"wait\", \n",
    "    \"but i'm not sure\", \"let's see if\", \"notice that\", \n",
    "    \"the final answer is\", \"let's assume\", \"we can conclude\",\n",
    "    \"implies that\", \"to solve this\", \"break it down\", \n",
    "    \"suppose that\", \"checking the\", \"recall that\"\n",
    "]\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward = 1.0 if the final answer is correct, 0.0 otherwise.\n",
    "    This is the \"Ground Truth\" signal.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion, correct_ans in zip(completions, answer):\n",
    "        # Simple check: is the answer roughly in the text?\n",
    "        # In a real system, you'd extract the number exactly.\n",
    "        # For now, we check if the correct string appears in the output.\n",
    "        if str(correct_ans) in completion:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "def hicra_planning_reward_func(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    HICRA Proxy: Gives a small bonus for using 'Strategic Grams'.\n",
    "    This encourages the model to 'think' before answering.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        score = 0.0\n",
    "        # Check for presence of planning words\n",
    "        completion_lower = completion.lower()\n",
    "        for gram in STRATEGIC_GRAMS:\n",
    "            if gram in completion_lower:\n",
    "                score += 0.1 # Small bonus for EACH planning step\n",
    "        \n",
    "        # Cap the bonus so it doesn't game the system just by spamming words\n",
    "        rewards.append(min(score, 0.5)) \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb281a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Prepare Data for GRPO\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the file you generated with the API script\n",
    "dataset = load_dataset(\"json\", data_files=\"reasoning_dataset.json\", split=\"train\")\n",
    "\n",
    "# GRPO expects a specific format. We don't need a system prompt for simple math.\n",
    "# It just needs 'prompt' and 'answer' (which we generated).\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Train with GRPO\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "# Load the TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard pointing to your output directory\n",
    "# (Make sure 'gemma-3-reasoning-output' matches the 'output_dir' in your GRPOConfig!)\n",
    "%tensorboard --logdir gemma-3-reasoning-output\n",
    "\n",
    "# Configuration for 12GB VRAM\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"gemma-3-reasoning-output\",\n",
    "    learning_rate=2e-5, # RL usually needs lower LR\n",
    "    per_device_train_batch_size=1, # Keep small for VRAM\n",
    "    gradient_accumulation_steps=8, \n",
    "    max_prompt_length=512,\n",
    "    max_completion_length=512, # Allow it to think!\n",
    "    num_generations=2, # Generate 2 answers per question to compare\n",
    "    max_steps=200, # Quick run to test\n",
    "    use_vllm=False,\n",
    "    save_steps=50,\n",
    "    logging_steps=1,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[correctness_reward_func, hicra_planning_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start the RL Loop!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402268d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from step 60 to step 180\n",
    "trainer.args.max_steps = 180  # New target\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from step 60 to step 180\n",
    "trainer.args.max_steps = 270  # New target\n",
    "\n",
    "# Resume from the last checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab47cb9",
   "metadata": {},
   "source": [
    "Set up the transformers inference API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b086751",
   "metadata": {},
   "source": [
    "1. Adjusting Your Script for the Project\n",
    "Here is the adjusted script. I have updated it to fit the Gemma-9B context and added a safety step to clear memory before merging (crucial on cloud GPUs to avoid crashing at the finish line).\n",
    "\n",
    "You should append this to the end of your training notebook/script.\n",
    "\n",
    "2. Important Step for HF Spaces\n",
    "You must add your Hugging Face Token as a Secret in the Space settings, or the script won't be able to push the model.\n",
    "\n",
    "Go to your Space -> Settings.\n",
    "\n",
    "Scroll to \"Variables and secrets\".\n",
    "\n",
    "Add a New Secret: HF_TOKEN -> [Paste your Write token]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- 1. MEMORY CLEANUP (Crucial for Cloud) ---\n",
    "# RL Training fills VRAM. We need to clear it before the heavy \"Merge\" step.\n",
    "print(\"üßπ Cleaning up VRAM before merging...\")\n",
    "try:\n",
    "    del trainer\n",
    "    del batch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# --- 2. RELOAD MODEL FOR MERGING ---\n",
    "# Sometimes it's safer to reload the base model + adapter freshly to merge\n",
    "# independent of the messy training state.\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"üîÑ Reloading model for clean merge...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-9b-it-bnb-4bit\", # Your base model\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Load the adapters you just trained\n",
    "# Assuming your GRPOConfig output_dir was \"gemma-reasoning-output\"\n",
    "# and the latest checkpoint is saved there.\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"gemma-reasoning-output/checkpoint-final\") # Update path to your actual checkpoint folder!\n",
    "\n",
    "# --- 3. LOGIN & PUSH ---\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No HF_TOKEN found! Check your Space 'Settings' -> 'Variables' to add it.\")\n",
    "\n",
    "repo_name = \"david-barnes/Gemma-2-9B-Reasoning-v1\" # Your new repo name\n",
    "\n",
    "print(f\"‚è≥ Merging to 16-bit and Pushing to: {repo_name}...\")\n",
    "\n",
    "# This takes care of the de-quantization and merging in one go\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\", # 16-bit is best for sharing reasoning models\n",
    "    token = hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Success! Your reasoning model is live.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed046041",
   "metadata": {},
   "source": [
    "### 3. Configure LoRA:\n",
    "\n",
    "Unsloth handles the target modules automatically (including the tricky gate_proj, up_proj, etc. that vanilla Peft requires you to list manually)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde9498",
   "metadata": {},
   "source": [
    "### Check where the model is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098082b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where the model is cached\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/\")\n",
    "print(f\"Model cache location: {cache_dir}\")\n",
    "print(\"\\nContents:\")\n",
    "if os.path.exists(cache_dir):\n",
    "    for item in os.listdir(cache_dir)[:10]:  # Show first 10 items\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"Cache directory not found yet\")\n",
    "\n",
    "# You can also set a custom cache location if you prefer:\n",
    "# os.environ['HF_HOME'] = '/path/to/custom/cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc77db5",
   "metadata": {},
   "source": [
    "## Apply QLora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381ced9",
   "metadata": {},
   "source": [
    "Quick calculation:\n",
    "\n",
    "700 records\n",
    "Effective batch size = per_device_batch_size (2) √ó gradient_accumulation_steps (4) = 8\n",
    "Steps per epoch = 700 / 8 = ~88 steps\n",
    "So 60 steps = ~0.7 epochs - you haven't even completed one full pass through your data yet!\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "Epochs |\tSteps |\tUse Case |\n",
    "1 |\t~90 |\tMinimum - sees all data once |\n",
    "2-3 |\t~180-270|\tSweet spot for fine-tuning |\n",
    "5+ |\t440+ |\tRisk of overfitting |\n",
    "\n",
    "Since your loss was still decreasing at step 60, you probably have room to train more. I'd suggest trying max_steps = 180 (about 2 epochs) for a good balance.\n",
    "\n",
    "Watch for:\n",
    "\n",
    "‚úÖ Good sign: Loss continues decreasing smoothly\n",
    "‚ö†Ô∏è Overfitting warning: Loss drops very low (<0.1) or starts fluctuating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db7b43",
   "metadata": {},
   "source": [
    "### LOGIN TO HUB\n",
    "\n",
    "When we push to HuggingFace Hub, it will merge our local QLoRa adaptor with the base model we used to train, on the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Try to login with token from environment variable\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "\tlogin(token=hf_token)\n",
    "\tprint(\"Logged in with HF_TOKEN environment variable\") \n",
    "else:\n",
    "\t# Skip login for local training - you can still train without pushing to hub\n",
    "\tprint(\"No HF_TOKEN found. Proceeding with local training on local GPU...\")\n",
    "\tprint(\"Note: You won't be able to push models to HuggingFace Hub without authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fb190",
   "metadata": {},
   "source": [
    "# Push Model to hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52519998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "# Point this to the exact folder on your disk\n",
    "checkpoint_path = \"outputs/checkpoint-180\" \n",
    "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\" # Your Hugging Face repo\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# 2. LOAD SPECIFIC CHECKPOINT\n",
    "# Unsloth is smart: if you point it to a folder, it loads the base model \n",
    "# AND applies the adapters from that folder automatically.\n",
    "print(f\"üìÇ Loading checkpoint from {checkpoint_path}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = checkpoint_path, \n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True, # Keep True for fast loading (Unsloth handles the merge magic)\n",
    ")\n",
    "\n",
    "# 3. MERGE & PUSH\n",
    "# This will de-quantize the base model, merge your checkpoint-180 adapters, \n",
    "# and upload a clean 16-bit model to the Hub.\n",
    "print(f\"üöÄ Merging and pushing to {repo_name}...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    repo_name,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
    "    token = hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Done! Your Junior Accountant (Checkpoint 180) is live!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Unsloth)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
