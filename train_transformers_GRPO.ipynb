{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "1f339f87",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# Force unsloth to use the local GPU memory efficiently\n",
                "# os.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\"\n",
                "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "fb9cc97a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Note: you may need to restart the kernel to use updated packages.\n",
                        "Collecting httpx==0.27.2\n",
                        "  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
                        "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 76 kB 18.8 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: anyio in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (4.12.0)\n",
                        "Collecting sniffio\n",
                        "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
                        "Requirement already satisfied: certifi in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (2021.5.30)\n",
                        "Requirement already satisfied: idna in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (2.10)\n",
                        "Requirement already satisfied: httpcore==1.* in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (1.0.9)\n",
                        "Requirement already satisfied: h11>=0.16 in /home/user/miniconda/lib/python3.9/site-packages (from httpcore==1.*->httpx==0.27.2) (0.16.0)\n",
                        "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/user/miniconda/lib/python3.9/site-packages (from anyio->httpx==0.27.2) (1.3.1)\n",
                        "Requirement already satisfied: typing_extensions>=4.5 in /home/user/miniconda/lib/python3.9/site-packages (from anyio->httpx==0.27.2) (4.15.0)\n",
                        "Installing collected packages: sniffio, httpx\n",
                        "  Attempting uninstall: httpx\n",
                        "    Found existing installation: httpx 0.28.1\n",
                        "    Uninstalling httpx-0.28.1:\n",
                        "      Successfully uninstalled httpx-0.28.1\n",
                        "Successfully installed httpx-0.27.2 sniffio-1.3.1\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "# run `setup_grpo_transformers.sh` first\n",
                "\n",
                "# OR Install required packages (needed after Space restarts)\n",
                "%pip install -q huggingface_hub transformers trl peft accelerate bitsandbytes datasets tensorboard dotenv\n",
                "%pip install httpx==0.27.2"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "587d407d",
            "metadata": {},
            "source": [
                "### Local login, not for use with spaces"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "af58fc9b",
            "metadata": {},
            "source": [
                "### Server-Side HF Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "17012abc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Token set! Restart remote shell to activate.\n"
                    ]
                }
            ],
            "source": [
                "# Set remote HF_TOKEN from local .env\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "hf_token = os.getenv('HF_TOKEN')\n",
                "\n",
                "# ssh -i ~/.ssh/id_ed25519 dataimaginations-heirarchical-reasoning@ssh.hf.space \"echo 'export HF_TOKEN={hf_token}' >> ~/.bashrc\"\n",
                "print(\"‚úÖ Token set! Restart remote shell to activate.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "7a4343d1",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "54342cb0b38f40ada479a10f0198b44f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Logged in with HF_TOKEN environment variable\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from huggingface_hub import login\n",
                "\n",
                "# Login using your HF token\n",
                "hf_token = os.getenv('HF_TOKEN')  # Try environment variable first\n",
                "\n",
                "if hf_token:\n",
                "    # login(token=hf_token)\n",
                "    login()\n",
                "    print(\"‚úÖ Logged in with HF_TOKEN environment variable\")\n",
                "else:\n",
                "    # If no env var, prompt for token (you'll need to paste it)\n",
                "    login()\n",
                "    print(\"‚úÖ Logged in interactively\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "437aea4d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_472/608623685.py:4: FutureWarning: Support for Python 3.9 will be dropped in the next release (after its end-of-life on October 31, 2025). Please upgrade to Python 3.10 or newer.\n",
                        "  from trl import GRPOConfig, GRPOTrainer\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚è≥ Loading model in 4-bit...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9891c13fd3f24c8d8042ff5053e94894",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "810bb3f165b046dc97e0a1a1e69e2aaa",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a51ebc5a98e24ac7b0d1326fef941db0",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1d581139be604596bd39c701ad232b2a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "37e31fbb485c4125b46fc8b6a952505f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4b274e63233a4f73be151a9fde964d83",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Loaded meta-llama/Llama-3.2-1B\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "from trl import GRPOConfig, GRPOTrainer\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "MODEL_NAME = \"meta-llama/Llama-3.2-1B\" # \"google/gemma-2-2b-it\"\n",
                "output_dir = \"llama-1b-reasoning-v1\"\n",
                "\n",
                "print(\"‚è≥ Loading model in 4-bit...\")\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    # No attn_implementation - let it auto-select\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# Fix padding\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    model.config.pad_token_id = tokenizer.pad_token_id\n",
                "\n",
                "print(f\"‚úÖ Loaded {MODEL_NAME}\")# Changed from flash_attention_2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "1d29e3d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Force the input embeddings to require grads (connects the frozen model to the trainable adapters)\n",
                "model.enable_input_require_grads()\n",
                "\n",
                "# Prepare Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "tokenizer.padding_side = \"left\" # CRITICAL for reasoning/generation steps!\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "4cc17269",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Pad token set to: <|end_of_text|>\n",
                        "‚úÖ Vocab size: 128256\n",
                        "‚úÖ Model vocab size: 128256\n"
                    ]
                }
            ],
            "source": [
                "# Fix tokenizer padding\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    model.config.pad_token_id = tokenizer.pad_token_id\n",
                "\n",
                "print(f\"‚úÖ Pad token set to: {tokenizer.pad_token}\")\n",
                "print(f\"‚úÖ Vocab size: {len(tokenizer)}\")\n",
                "\n",
                "# Gemma3 has a nested config structure (vision + text)\n",
                "if hasattr(model.config, 'text_config'):\n",
                "    print(f\"‚úÖ Model vocab size: {model.config.text_config.vocab_size}\")\n",
                "else:\n",
                "    print(f\"‚úÖ Model vocab size: {model.config.vocab_size}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a851bcc9",
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import GRPOTrainer\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class HICRAGRPOTrainer(GRPOTrainer):\n",
                "    def __init__(self, hicra_manager, tokenizer, *args, **kwargs):\n",
                "        super().__init__(*args, **kwargs)\n",
                "        self.hicra_manager = hicra_manager\n",
                "        self.tokenizer = tokenizer\n",
                "\n",
                "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
                "        \"\"\"\n",
                "        Overridden to inject HICRA Token-Level Advantage Modification.\n",
                "        \"\"\"\n",
                "        # 1. Standard GRPO Forward Pass (to get completions and per-token KL)\n",
                "        # We assume the parent class handles the rollout generation and padding\n",
                "        # This part relies on TRL's internal structure. \n",
                "        # Since TRL 0.8+ is complex, we will hook into the advantage calculation step \n",
                "        # if possible, or post-process the advantages.\n",
                "        \n",
                "        # NOTE: For stability in a Notebook, we will use the standard parent compute_loss\n",
                "        # but we will use a PyTorch hook to modify the advantages on the fly \n",
                "        # if the library supports it. \n",
                "        \n",
                "        # However, since simpler is better for crashing notebooks, let's try a \n",
                "        # cleaner approach: The \"HICRA Loss Wrapper\".\n",
                "        \n",
                "        return super().compute_loss(model, inputs, return_outputs)\n",
                "\n",
                "    # --- THE MAGIC METHOD ---\n",
                "    # TRL's GRPOTrainer usually computes advantages internally. \n",
                "    # To implement HICRA properly without breaking the library versions,\n",
                "    # we can pass the HICRA logic as a \"Reward Function\" that returns\n",
                "    # token-level scores, but GRPO expects scalar rewards.\n",
                "    \n",
                "    # STRATEGY CHANGE:\n",
                "    # Since subclassing the massive compute_loss is prone to breaking with version updates,\n",
                "    # we will use the HICRA_Manager you have, but apply it as a \"Weighted Mask\" \n",
                "    # on the loss, which is mathematically very similar.\n",
                "    \n",
                "    pass "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "2a08ae03",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: The HICRA Logic (Strategic Grams)\n",
                "import torch\n",
                "import re\n",
                "from tqdm import tqdm\n",
                "from torch.optim import AdamW\n",
                "import torch.nn.functional as F\n",
                "\n",
                "# Global for compatibility with the trainer's reward_funcs if needed\n",
                "STRATEGIC_GRAMS = [\n",
                "    \"first i need to\", \"let's look at\", \"alternatively\", \"wait\", \n",
                "    \"but i'm not sure\", \"let's see if\", \"notice that\", \n",
                "    \"the final answer is\", \"let's assume\", \"we can canclude\",\n",
                "    \"implies that\", \"to solve this\", \"break it down\", \n",
                "    \"suppose that\", \"checking the\", \"recall that\"\n",
                "]\n",
                "\n",
                "class HICRA_Manager:\n",
                "    def __init__(self, alpha=0.2):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            alpha (float): The amplification factor (paper uses 0.2)[cite: 574].\n",
                "        \"\"\"\n",
                "        self.alpha = alpha\n",
                "        \n",
                "        # A subset of \"Strategic Grams\" from Listing 1 in the paper.\n",
                "        # These function as 'planning tokens'.\n",
                "        self.strategic_grams = [\n",
                "            \"first i need to\", \"let's look at\", \"alternatively\", \"wait\", \n",
                "            \"but i'm not sure\", \"let's see if\", \"notice that\", \n",
                "            \"the final answer is\", \"let's assume\", \"we can canclude\",\n",
                "            \"implies that\", \"to solve this\", \"break it down\", \n",
                "            \"suppose that\", \"checking the\", \"recall that\"\n",
                "        ]\n",
                "        \n",
                "    def identify_planning_mask(self, input_ids, tokenizer):\n",
                "        \"\"\"\n",
                "        Identifies which tokens in a sequence belong to a 'Strategic Gram'.\n",
                "        Returns a boolean mask of shape (batch_size, seq_len).\n",
                "        \"\"\"\n",
                "        batch_size, seq_len = input_ids.shape\n",
                "        planning_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
                "        \n",
                "        # Decode to text to find phrases (simplified approach)\n",
                "        texts = tokenizer.batch_decode(input_ids, skip_special_tokens=False)\n",
                "        \n",
                "        for b_idx, text in enumerate(texts):\n",
                "            # Simple heuristic: Check if the text contains the gram\n",
                "            for gram in self.strategic_grams:\n",
                "                # Find all occurrences of the gram in the text\n",
                "                for match in re.finditer(re.escape(gram), text, re.IGNORECASE):\n",
                "                    start_char, end_char = match.span()\n",
                "                    \n",
                "                    # Convert char positions to token positions using the tokenizer\n",
                "                    encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
                "                    offsets = encoding['offset_mapping']\n",
                "                    \n",
                "                    for t_idx, (t_start, t_end) in enumerate(offsets):\n",
                "                        # If token falls within the matched gram\n",
                "                        if t_end > start_char and t_start < end_char:\n",
                "                            # Handle truncation/padding indices safely\n",
                "                            if t_idx < seq_len:\n",
                "                                planning_mask[b_idx, t_idx] = True\n",
                "                                \n",
                "        return planning_mask\n",
                "\n",
                "    def compute_hicra_advantages(self, advantages, planning_mask):\n",
                "        \"\"\"\n",
                "        Applies the HICRA modification to the advantages.\n",
                "        \"\"\"\n",
                "        hicra_adjustment = self.alpha * advantages.abs()\n",
                "        \n",
                "        # Apply mask: only adjust planning tokens\n",
                "        hicra_adjustment = hicra_adjustment * planning_mask.float()\n",
                "        \n",
                "        # Final HICRA advantages\n",
                "        hicra_advantages = advantages + hicra_adjustment\n",
                "        \n",
                "        return hicra_advantages\n",
                "\n",
                "# Keep helper functions for potential backward compatibility\n",
                "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
                "    rewards = []\n",
                "    for completion in completions:\n",
                "        reward = 1.0 if str(answer).strip() in completion.strip() else 0.0\n",
                "        rewards.append(reward)\n",
                "    return rewards\n",
                "\n",
                "def hicra_planning_reward_func(prompts, completions, **kwargs):\n",
                "    rewards = []\n",
                "    for completion in completions:\n",
                "        score = 0.0\n",
                "        completion_lower = completion.lower()\n",
                "        for gram in STRATEGIC_GRAMS:\n",
                "            if gram in completion_lower:\n",
                "                score += 0.1 \n",
                "        rewards.append(min(score, 0.5)) \n",
                "    return rewards\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "eb281a58",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b72360a2bb30489f81c8f01febb4e137",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split: 0 examples [00:00, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "14176866cfd24a148754c8e61b28969e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split: 0 examples [00:00, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'prompt': 'In a survey of 200 students, it was found that x students like mathematics, y students like physics, and z students like chemistry. The number of students who like exactly two subjects is 45, and the number who like all three subjects is 12. If 25 students like both math and physics but not chemistry, 18 students like both physics and chemistry but not math, and 22 students like both math and chemistry but not physics, find the value of x + y + z given that exactly 30 students like none of the three subjects.', 'answer': '312'}\n"
                    ]
                }
            ],
            "source": [
                "# 1. Load BOTH Datasets\n",
                "from datasets import load_dataset\n",
                "\n",
                "dataset_train = load_dataset(\"json\", data_files=\"reasoning_dataset_v2_train.json\", split=\"train\", download_mode=\"force_redownload\")\n",
                "dataset_test = load_dataset(\"json\", data_files=\"reasoning_dataset_v2_test.json\", split=\"train\", download_mode=\"force_redownload\") # Load as 'train' split then rename if needed, but 'train' split works fine for just passing data\n",
                "# For later: nvidia/Nemotron-Post-Training-Dataset-v1\n",
                "\n",
                "# GRPO expects a specific format. We don't need a system prompt for simple math.\n",
                "# It just needs 'prompt' and 'answer' (which we generated).\n",
                "print(dataset_train[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "f1d0ce3d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sample data:\n",
                        "{'prompt': 'In a survey of 200 students, it was found that x students like mathematics, y students like physics, and z students like chemistry. The number of students who like exactly two subjects is 45, and the number who like all three subjects is 12. If 25 students like both math and physics but not chemistry, 18 students like both physics and chemistry but not math, and 22 students like both math and chemistry but not physics, find the value of x + y + z given that exactly 30 students like none of the three subjects.', 'answer': '312'}\n",
                        "\n",
                        "Keys: dict_keys(['prompt', 'answer'])\n",
                        "\n",
                        "Prompt type: <class 'str'>\n",
                        "Answer type: <class 'str'>\n"
                    ]
                }
            ],
            "source": [
                "# Test your dataset format\n",
                "print(\"Sample data:\")\n",
                "print(dataset_train[0])\n",
                "print(\"\\nKeys:\", dataset_train[0].keys())\n",
                "print(\"\\nPrompt type:\", type(dataset_train[0]['prompt']))\n",
                "print(\"Answer type:\", type(dataset_train[0]['answer']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "2c57e8db",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîó Attaching LoRA adapters...\n"
                    ]
                }
            ],
            "source": [
                "# Attach LoRA Adapters (PEFT)\n",
                "print(\"üîó Attaching LoRA adapters...\")\n",
                "peft_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                ")\n",
                "# We wrap the model manually so GRPO knows it's a PEFT model\n",
                "model = get_peft_model(model, peft_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "memory_patch_123",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Applied memory optimization patch to pooling layer!\n"
                    ]
                }
            ],
            "source": [
                "# --- MEMORY OPTIMIZATION MONKEY PATCH ---\n",
                "# This fixes the OOM error by processing log-probs in mini-batches\n",
                "import trl.trainer.grpo_trainer\n",
                "import torch\n",
                "\n",
                "def get_per_token_logps_chunked(model, input_ids, num_logits_to_keep, mini_batch_size=1):\n",
                "    per_token_logps = []\n",
                "    batch_size = input_ids.size(0)\n",
                "    for i in range(0, batch_size, mini_batch_size):\n",
                "        batch_end = min(i + mini_batch_size, batch_size)\n",
                "        mini_input_ids = input_ids[i:batch_end]\n",
                "        # The standard implementation calculates all logits at once, causing OOM\n",
                "        # We do it in chunks:\n",
                "        mini_logits = model(mini_input_ids, num_logits_to_keep=num_logits_to_keep + 1).logits\n",
                "        mini_logits = mini_logits[:, :-1, :] # exclude last logit\n",
                "\n",
                "        # Compute log probs\n",
                "        log_probs = mini_logits.log_softmax(dim=-1)\n",
                "        labels = mini_input_ids[:, -num_logits_to_keep:].unsqueeze(2)\n",
                "        token_log_prob = torch.gather(log_probs, dim=2, index=labels).squeeze(2)\n",
                "        per_token_logps.append(token_log_prob)\n",
                "    return torch.cat(per_token_logps, dim=0)\n",
                "\n",
                "# Apply the patch to the library function\n",
                "# We set mini_batch_size=1 to be extremely safe with memory\n",
                "trl.trainer.grpo_trainer.get_per_token_logps = lambda model, input_ids, num_logits_to_keep: get_per_token_logps_chunked(model, input_ids, num_logits_to_keep, mini_batch_size=1)\n",
                "print(\"‚úÖ Applied memory optimization patch to pooling layer!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "3452d607",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the TensorBoard extension\n",
                "# %load_ext tensorboard\n",
                "# %tensorboard --logdir gemma-2-2b-reasoning-v2 --port 6006 --bind_all\n",
                "# Start TensorBoard pointing to your output directory\n",
                "# (Make sure 'gemma-3-reasoning-output' matches the 'output_dir' in your GRPOConfig!)\n",
                "# %tensorboard --logdir gemma-2-2b-reasoning-output\n",
                "\n",
                "# Define Training Arguments (GRPO)\n",
                "training_args = GRPOConfig(\n",
                "    output_dir=\"llama-1b-reasoning-v1\",\n",
                "    learning_rate=5e-6,\n",
                "    per_device_train_batch_size=1,\n",
                "    per_device_eval_batch_size=2, # Reverted to 2 (must be divisible by num_generations)\n",
                "    gradient_accumulation_steps=4,\n",
                "    gradient_checkpointing=True,\n",
                "    max_prompt_length=256,\n",
                "    max_completion_length=200, # Reduced to save memory\n",
                "    num_generations=2, # Reduced to 2 to prevent OOM\n",
                "    \n",
                "    # --- STEP MATH SETTINGS ---\n",
                "    max_steps=330, \n",
                "    warmup_steps=30, # ~10% of total steps\n",
                "    \n",
                "    # --- VALIDATION SETTINGS ---\n",
                "    eval_strategy=\"steps\",      # Check validation set every X steps\n",
                "    eval_steps=10,              # Check every 50 steps\n",
                "    save_steps=10,              # Save a checkpoint every 50 steps\n",
                "    logging_steps=2,\n",
                "    \n",
                "    fp16=False,\n",
                "    bf16=True,\n",
                "    report_to=\"tensorboard\"\n",
                ")\n",
                "# Force the input embeddings to require grads (connects the frozen model to the trainable adapters)\n",
                "# model.enable_input_require_grads()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "0f43ca0b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Starting GRPO Trainer...\n"
                    ]
                }
            ],
            "source": [
                "# 5. Initialize Trainer\n",
                "# Note: We assume you still have your 'dataset' and reward functions from before\n",
                "print(\"üöÄ Starting GRPO Trainer...\")\n",
                "trainer = GRPOTrainer(\n",
                "    model=model,\n",
                "    processing_class=tokenizer,\n",
                "    reward_funcs=[correctness_reward_func, hicra_planning_reward_func],\n",
                "    args=training_args,\n",
                "    train_dataset=dataset_train, # Your 630 questions\n",
                "    eval_dataset=dataset_test,   # Your 50 test questions <--- ADDED THIS\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "adbe9716",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Starting Manual HICRA Training Loop (Updated V2)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|‚ñã                                                                                                                                                                  | 3/729 [00:53<3:37:06, 17.94s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Step 3: Loss=-2.4257, Mean Reward=-0.50\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  1%|‚ñà‚ñå                                                                                                                                                                 | 7/729 [01:47<2:46:53, 13.87s/it]"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mFailed to restart the Kernel. \n",
                        "\u001b[1;31mrequest to http://localhost:8888/api/kernels/d94017d5-8f8e-4d42-9d70-97004d6af197/restart?1766692003202 failed, reason: connect ECONNREFUSED 127.0.0.1:8888. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "# 1. Setup HICRA Manager (Using your code)\n",
                "import gc\n",
                "hicra = HICRA_Manager(alpha=0.2) # Alpha from paper \n",
                "\n",
                "# 2. Optimizer\n",
                "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
                "\n",
                "# 3. Manual Training Loop (Replaces trainer.train())\n",
                "print(\"üöÄ Starting Manual HICRA Training Loop (Updated V2)...\")\n",
                "\n",
                "# CRITICAL STABILIZATION FIXES:\n",
                "model.gradient_checkpointing_disable() # Llama-1B on L40S doesn't need this, and it causes crashes\n",
                "model.train()\n",
                "\n",
                "EPOCHS = 2\n",
                "GRAD_ACCUM_STEPS = 4\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    for step, batch in enumerate(tqdm(dataset_train)):\n",
                "        \n",
                "        # A. Format Input\n",
                "        prompt = batch['prompt']\n",
                "        answer = batch['answer']\n",
                "        \n",
                "        # B. Generate Rollouts\n",
                "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "        \n",
                "        try:\n",
                "            with torch.no_grad():\n",
                "                # Enable autocast for mixed precision (bf16) to avoid type mismatches\n",
                "                with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
                "                    outputs = model.generate(\n",
                "                        **inputs,\n",
                "                        max_new_tokens=300,\n",
                "                        do_sample=True,\n",
                "                        temperature=0.9,\n",
                "                        num_return_sequences=4,\n",
                "                        pad_token_id=tokenizer.pad_token_id\n",
                "                    )\n",
                "            \n",
                "            # C. Score the Outputs\n",
                "            rewards = []\n",
                "            generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
                "            \n",
                "            for text in generated_texts:\n",
                "                r = 1.0 if str(answer) in text else -1.0\n",
                "                rewards.append(r)\n",
                "            \n",
                "            rewards = torch.tensor(rewards, device=model.device)\n",
                "            \n",
                "            # D. Compute Standard GRPO Advantages\n",
                "            mean_r = rewards.mean()\n",
                "            std_r = rewards.std() + 1e-8\n",
                "            advantages = (rewards - mean_r) / std_r\n",
                "            \n",
                "            # --- E. APPLY HICRA (Updated Method) ---\n",
                "            # 1. Identify Planning Tokens\n",
                "            planning_mask = hicra.identify_planning_mask(outputs, tokenizer).to(model.device)\n",
                "            \n",
                "            # 2. Modify Advantages\n",
                "            token_advantages = advantages.view(-1, 1).expand_as(planning_mask).clone()\n",
                "            \n",
                "            # Use the class method for advantage adjustment\n",
                "            final_advantages = hicra.compute_hicra_advantages(token_advantages, planning_mask)\n",
                "            \n",
                "            # F. Compute Policy Gradient Loss\n",
                "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
                "                logits = model(outputs).logits\n",
                "                shift_logits = logits[..., :-1, :].contiguous()\n",
                "                shift_labels = outputs[..., 1:].contiguous()\n",
                "                log_probs = F.log_softmax(shift_logits, dim=-1)\n",
                "                token_log_probs = torch.gather(log_probs, 2, shift_labels.unsqueeze(2)).squeeze(2)\n",
                "                \n",
                "                prompt_len = inputs.input_ids.shape[1]\n",
                "                loss_mask = torch.ones_like(token_log_probs)\n",
                "                loss_mask[:, :prompt_len] = 0\n",
                "                \n",
                "                # Mask alignment\n",
                "                current_advantages = final_advantages[:, 1:]\n",
                "                min_len = min(current_advantages.shape[1], token_log_probs.shape[1])\n",
                "                current_advantages = current_advantages[:, :min_len]\n",
                "                token_log_probs = token_log_probs[:, :min_len]\n",
                "                loss_mask = loss_mask[:, :min_len]\n",
                "                \n",
                "                loss = - (current_advantages * token_log_probs * loss_mask).sum() / loss_mask.sum()\n",
                "            \n",
                "            # G. Optimization Step\n",
                "            loss = loss / GRAD_ACCUM_STEPS\n",
                "            loss.backward()\n",
                "            \n",
                "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
                "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "                optimizer.step()\n",
                "                optimizer.zero_grad()\n",
                "                print(f\"Step {step}: Loss={loss.item()*GRAD_ACCUM_STEPS:.4f}, Mean Reward={mean_r:.2f}\")\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"Error at step {step}: {e}\")\n",
                "            # Don't break, try to clean up and continue (optional, but good for debugging)\n",
                "            continue\n",
                "\n",
                "        # H. VRAM Cleanup\n",
                "        del outputs, logits, loss\n",
                "        gc.collect()\n",
                "        torch.cuda.empty_cache()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "402268d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Continue training from step 60 to step 180\n",
                "trainer.args.max_steps = 180  # New target\n",
                "\n",
                "# Resume from the last checkpoint\n",
                "trainer_stats = trainer.train(resume_from_checkpoint=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2b5d57da",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Continue training from step 60 to step 180\n",
                "trainer.args.max_steps = 270  # New target\n",
                "\n",
                "# Resume from the last checkpoint\n",
                "trainer_stats = trainer.train(resume_from_checkpoint=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4ab47cb9",
            "metadata": {},
            "source": [
                "Set up the transformers inference API:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b23e612",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import os\n",
                "import gc\n",
                "from huggingface_hub import login\n",
                "\n",
                "# --- 1. MEMORY CLEANUP (Crucial for Cloud) ---\n",
                "# RL Training fills VRAM. We need to clear it before the heavy \"Merge\" step.\n",
                "print(\"üßπ Cleaning up VRAM before merging...\")\n",
                "try:\n",
                "    del trainer\n",
                "    del batch\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "except:\n",
                "    pass\n",
                "\n",
                "# --- 2. RELOAD MODEL FOR MERGING ---\n",
                "# Sometimes it's safer to reload the base model + adapter freshly to merge\n",
                "# independent of the messy training state.\n",
                "from unsloth import FastLanguageModel\n",
                "\n",
                "print(\"üîÑ Reloading model for clean merge...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"gemma-2-9b-reasoning-v1\", # Your base model\n",
                "    max_seq_length = 4096,\n",
                "    dtype = None,\n",
                "    load_in_4bit = True,\n",
                ")\n",
                "\n",
                "# Load the adapters you just trained\n",
                "# Assuming your GRPOConfig output_dir was \"gemma-reasoning-output\"\n",
                "# and the latest checkpoint is saved there.\n",
                "from peft import PeftModel\n",
                "model = PeftModel.from_pretrained(model, \"gemma-reasoning-output/checkpoint-final\") # Update path to your actual checkpoint folder!\n",
                "\n",
                "# --- 3. LOGIN & PUSH ---\n",
                "hf_token = os.environ.get(\"HF_TOKEN\")\n",
                "if hf_token:\n",
                "    login(token=hf_token)\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No HF_TOKEN found! Check your Space 'Settings' -> 'Variables' to add it.\")\n",
                "\n",
                "repo_name = \"DataImaginations/Gemma-2-9B-Reasoning-v1\" # Repo name might be `DataImaginations/` or `david-barnes`\n",
                "\n",
                "print(f\"‚è≥ Merging to 16-bit and Pushing to: {repo_name}...\")\n",
                "\n",
                "# This takes care of the de-quantization and merging in one go\n",
                "model.push_to_hub_merged(\n",
                "    repo_name,\n",
                "    tokenizer,\n",
                "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
                "    token = hf_token\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Success! Your reasoning model is live.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "435fb190",
            "metadata": {},
            "source": [
                "# Push Model to hub!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "52519998",
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import os\n",
                "device = \"cuda:0\"\n",
                "\n",
                "# 1. CONFIGURATION\n",
                "# Point this to the exact folder on your disk\n",
                "checkpoint_path = \"outputs/checkpoint-180\" \n",
                "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\" # Your Hugging Face repo\n",
                "hf_token = os.getenv('HF_TOKEN')\n",
                "\n",
                "# 2. LOAD SPECIFIC CHECKPOINT\n",
                "# Unsloth is smart: if you point it to a folder, it loads the base model \n",
                "# AND applies the adapters from that folder automatically.\n",
                "print(f\"üìÇ Loading checkpoint from {checkpoint_path}...\")\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = checkpoint_path, \n",
                "    max_seq_length = 2048,\n",
                "    dtype = None,\n",
                "    load_in_4bit = True, # Keep True for fast loading (Unsloth handles the merge magic)\n",
                ")\n",
                "\n",
                "# 3. MERGE & PUSH\n",
                "# This will de-quantize the base model, merge your checkpoint-180 adapters, \n",
                "# and upload a clean 16-bit model to the Hub.\n",
                "print(f\"üöÄ Merging and pushing to {repo_name}...\")\n",
                "\n",
                "model.push_to_hub_merged(\n",
                "    repo_name,\n",
                "    tokenizer,\n",
                "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
                "    token = hf_token\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Done! Your Junior Accountant (Checkpoint 180) is live!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
