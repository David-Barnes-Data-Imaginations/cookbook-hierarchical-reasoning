{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "1f339f87",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# Force unsloth to use the local GPU memory efficiently\n",
                "# os.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\"\n",
                "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "fb9cc97a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Note: you may need to restart the kernel to use updated packages.\n",
                        "Collecting httpx==0.27.2\n",
                        "  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
                        "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 76 kB 10.0 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: anyio in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (4.12.0)\n",
                        "Requirement already satisfied: idna in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (2.10)\n",
                        "Requirement already satisfied: certifi in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (2021.5.30)\n",
                        "Requirement already satisfied: httpcore==1.* in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (1.0.9)\n",
                        "Collecting sniffio\n",
                        "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
                        "Requirement already satisfied: h11>=0.16 in /home/user/miniconda/lib/python3.9/site-packages (from httpcore==1.*->httpx==0.27.2) (0.16.0)\n",
                        "Requirement already satisfied: typing_extensions>=4.5 in /home/user/miniconda/lib/python3.9/site-packages (from anyio->httpx==0.27.2) (4.15.0)\n",
                        "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/user/miniconda/lib/python3.9/site-packages (from anyio->httpx==0.27.2) (1.3.1)\n",
                        "Installing collected packages: sniffio, httpx\n",
                        "  Attempting uninstall: httpx\n",
                        "    Found existing installation: httpx 0.28.1\n",
                        "    Uninstalling httpx-0.28.1:\n",
                        "      Successfully uninstalled httpx-0.28.1\n",
                        "Successfully installed httpx-0.27.2 sniffio-1.3.1\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "# run `setup_grpo_transformers.sh` first\n",
                "\n",
                "# OR Install required packages (needed after Space restarts)\n",
                "%pip install -q huggingface_hub transformers trl peft accelerate bitsandbytes datasets tensorboard dotenv\n",
                "%pip install httpx==0.27.2"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "587d407d",
            "metadata": {},
            "source": [
                "### Local login, not for use with spaces"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "af58fc9b",
            "metadata": {},
            "source": [
                "### Server-Side HF Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "17012abc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Token set! Restart remote shell to activate.\n"
                    ]
                }
            ],
            "source": [
                "# Set remote HF_TOKEN from local .env\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "hf_token = os.getenv('HF_TOKEN')\n",
                "\n",
                "# ssh -i ~/.ssh/id_ed25519 dataimaginations-heirarchical-reasoning@ssh.hf.space \"echo 'export HF_TOKEN={hf_token}' >> ~/.bashrc\"\n",
                "print(\"‚úÖ Token set! Restart remote shell to activate.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "7a4343d1",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "790fed19efb349cea8b5344c0fe015d3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Logged in with HF_TOKEN environment variable\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from huggingface_hub import login\n",
                "\n",
                "# Login using your HF token\n",
                "hf_token = os.getenv('HF_TOKEN')  # Try environment variable first\n",
                "\n",
                "if hf_token:\n",
                "    # login(token=hf_token)\n",
                "    login()\n",
                "    print(\"‚úÖ Logged in with HF_TOKEN environment variable\")\n",
                "else:\n",
                "    # If no env var, prompt for token (you'll need to paste it)\n",
                "    login()\n",
                "    print(\"‚úÖ Logged in interactively\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "57a44ab7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# !ssh -i ~/.ssh/id_ed25519 dataimaginations-heirarchical-reasoning@ssh.hf.space \"echo 'export HF_TOKEN=' >> ~/.bashrc\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "437aea4d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_732/3638208885.py:4: FutureWarning: Support for Python 3.9 will be dropped in the next release (after its end-of-life on October 31, 2025). Please upgrade to Python 3.10 or newer.\n",
                        "  from trl import GRPOConfig, GRPOTrainer\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚è≥ Loading model in 4-bit...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c5f4c24374b94f15a4c40614e07b7c93",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "30cb93b8e6124ece97e02174132e47a1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "34f93c79df634e318ecde34540c9cdb1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cd81ed8ff9994e059722c4de46cab156",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "89f932c93d8b4fe8ab0a54e8daafb079",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "614ec5d4976d4fa5b9e9c7504c05a858",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5209e89b96034379bec1ee33ad208fec",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "66c96dbb42c84677ab9bb14fee8d94f7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "73ccdf4330b2477d90deab32bba0f561",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "db2e9b15eb1f49ee9b792d09cc3ca29d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5b89043f96ab4a5a90d84dbedafeef56",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Loaded google/gemma-2-2b-it\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "from trl import GRPOConfig, GRPOTrainer\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
                "output_dir = \"gemma-2-2b-reasoning-v1\"\n",
                "\n",
                "print(\"‚è≥ Loading model in 4-bit...\")\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    # No attn_implementation - let it auto-select\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# Fix padding\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    model.config.pad_token_id = tokenizer.pad_token_id\n",
                "\n",
                "print(f\"‚úÖ Loaded {MODEL_NAME}\")# Changed from flash_attention_2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "1d29e3d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Prepare Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "tokenizer.padding_side = \"left\" # CRITICAL for reasoning/generation steps!\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "4cc17269",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Pad token set to: <pad>\n",
                        "‚úÖ Vocab size: 256000\n",
                        "‚úÖ Model vocab size: 256000\n"
                    ]
                }
            ],
            "source": [
                "# Fix tokenizer padding\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    model.config.pad_token_id = tokenizer.pad_token_id\n",
                "\n",
                "print(f\"‚úÖ Pad token set to: {tokenizer.pad_token}\")\n",
                "print(f\"‚úÖ Vocab size: {len(tokenizer)}\")\n",
                "\n",
                "# Gemma3 has a nested config structure (vision + text)\n",
                "if hasattr(model.config, 'text_config'):\n",
                "    print(f\"‚úÖ Model vocab size: {model.config.text_config.vocab_size}\")\n",
                "else:\n",
                "    print(f\"‚úÖ Model vocab size: {model.config.vocab_size}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "2a08ae03",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: The HICRA Logic (Strategic Grams)\n",
                "\n",
                "# These are the \"thinking words\" the paper identified. \n",
                "# When the model uses these, it is \"planning\".\n",
                "STRATEGIC_GRAMS = [\n",
                "    \"first i need to\", \"let's look at\", \"alternatively\", \"wait\", \n",
                "    \"but i'm not sure\", \"let's see if\", \"notice that\", \n",
                "    \"the final answer is\", \"let's assume\", \"we can conclude\",\n",
                "    \"implies that\", \"to solve this\", \"break it down\", \n",
                "    \"suppose that\", \"checking the\", \"recall that\"\n",
                "]\n",
                "\n",
                "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
                "    \"\"\"\n",
                "    Give reward=1.0 if completion matches answer, 0.0 otherwise\n",
                "    \"\"\"\n",
                "    rewards = []\n",
                "    for completion in completions:\n",
                "        # Extract just the number from the completion\n",
                "        # (model might generate extra text like \"The answer is 14\")\n",
                "        reward = 1.0 if str(answer).strip() in completion.strip() else 0.0\n",
                "        rewards.append(reward)\n",
                "    return rewards\n",
                "\n",
                "def hicra_planning_reward_func(prompts, completions, **kwargs):\n",
                "    \"\"\"\n",
                "    HICRA Proxy: Gives a small bonus for using 'Strategic Grams'.\n",
                "    This encourages the model to 'think' before answering.\n",
                "    \"\"\"\n",
                "    rewards = []\n",
                "    for completion in completions:\n",
                "        score = 0.0\n",
                "        # Check for presence of planning words\n",
                "        completion_lower = completion.lower()\n",
                "        for gram in STRATEGIC_GRAMS:\n",
                "            if gram in completion_lower:\n",
                "                score += 0.1 # Small bonus for EACH planning step\n",
                "        \n",
                "        # Cap the bonus so it doesn't game the system just by spamming words\n",
                "        rewards.append(min(score, 0.5)) \n",
                "    return rewards"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "eb281a58",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "508cdcba4cbc4269b7ba8f84a528d0b8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split: 0 examples [00:00, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e2ca768f281c4805995307f216f2d6b5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split: 0 examples [00:00, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'prompt': 'In a survey of 200 students, it was found that x students like mathematics, y students like physics, and z students like chemistry. The number of students who like exactly two subjects is 45, and the number who like all three subjects is 12. If 25 students like both math and physics but not chemistry, 18 students like both physics and chemistry but not math, and 22 students like both math and chemistry but not physics, find the value of x + y + z given that exactly 30 students like none of the three subjects.', 'answer': '312'}\n"
                    ]
                }
            ],
            "source": [
                "# 1. Load BOTH Datasets\n",
                "from datasets import load_dataset\n",
                "\n",
                "dataset_train = load_dataset(\"json\", data_files=\"reasoning_dataset_v2_train.json\", split=\"train\", download_mode=\"force_redownload\")\n",
                "dataset_test = load_dataset(\"json\", data_files=\"reasoning_dataset_v2_test.json\", split=\"train\", download_mode=\"force_redownload\") # Load as 'train' split then rename if needed, but 'train' split works fine for just passing data\n",
                "# For later: nvidia/Nemotron-Post-Training-Dataset-v1\n",
                "\n",
                "# GRPO expects a specific format. We don't need a system prompt for simple math.\n",
                "# It just needs 'prompt' and 'answer' (which we generated).\n",
                "print(dataset_train[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "f1d0ce3d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sample data:\n",
                        "{'prompt': 'In a survey of 200 students, it was found that x students like mathematics, y students like physics, and z students like chemistry. The number of students who like exactly two subjects is 45, and the number who like all three subjects is 12. If 25 students like both math and physics but not chemistry, 18 students like both physics and chemistry but not math, and 22 students like both math and chemistry but not physics, find the value of x + y + z given that exactly 30 students like none of the three subjects.', 'answer': '312'}\n",
                        "\n",
                        "Keys: dict_keys(['prompt', 'answer'])\n",
                        "\n",
                        "Prompt type: <class 'str'>\n",
                        "Answer type: <class 'str'>\n"
                    ]
                }
            ],
            "source": [
                "# Test your dataset format\n",
                "print(\"Sample data:\")\n",
                "print(dataset_train[0])\n",
                "print(\"\\nKeys:\", dataset_train[0].keys())\n",
                "print(\"\\nPrompt type:\", type(dataset_train[0]['prompt']))\n",
                "print(\"Answer type:\", type(dataset_train[0]['answer']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "2c57e8db",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîó Attaching LoRA adapters...\n"
                    ]
                }
            ],
            "source": [
                "# Attach LoRA Adapters (PEFT)\n",
                "print(\"üîó Attaching LoRA adapters...\")\n",
                "peft_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                ")\n",
                "# We wrap the model manually so GRPO knows it's a PEFT model\n",
                "model = get_peft_model(model, peft_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "memory_patch_123",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Applied memory optimization patch to pooling layer!\n"
                    ]
                }
            ],
            "source": [
                "# --- MEMORY OPTIMIZATION MONKEY PATCH ---\n",
                "# This fixes the OOM error by processing log-probs in mini-batches\n",
                "import trl.trainer.grpo_trainer\n",
                "import torch\n",
                "\n",
                "def get_per_token_logps_chunked(model, input_ids, num_logits_to_keep, mini_batch_size=1):\n",
                "    per_token_logps = []\n",
                "    batch_size = input_ids.size(0)\n",
                "    for i in range(0, batch_size, mini_batch_size):\n",
                "        batch_end = min(i + mini_batch_size, batch_size)\n",
                "        mini_input_ids = input_ids[i:batch_end]\n",
                "        # The standard implementation calculates all logits at once, causing OOM\n",
                "        # We do it in chunks:\n",
                "        mini_logits = model(mini_input_ids, num_logits_to_keep=num_logits_to_keep + 1).logits\n",
                "        mini_logits = mini_logits[:, :-1, :] # exclude last logit\n",
                "\n",
                "        # Compute log probs\n",
                "        log_probs = mini_logits.log_softmax(dim=-1)\n",
                "        labels = mini_input_ids[:, -num_logits_to_keep:].unsqueeze(2)\n",
                "        token_log_prob = torch.gather(log_probs, dim=2, index=labels).squeeze(2)\n",
                "        per_token_logps.append(token_log_prob)\n",
                "    return torch.cat(per_token_logps, dim=0)\n",
                "\n",
                "# Apply the patch to the library function\n",
                "# We set mini_batch_size=1 to be extremely safe with memory\n",
                "trl.trainer.grpo_trainer.get_per_token_logps = lambda model, input_ids, num_logits_to_keep: get_per_token_logps_chunked(model, input_ids, num_logits_to_keep, mini_batch_size=1)\n",
                "print(\"‚úÖ Applied memory optimization patch to pooling layer!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "3452d607",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the TensorBoard extension\n",
                "# %load_ext tensorboard\n",
                "# %tensorboard --logdir gemma-2-2b-reasoning-v2 --port 6006 --bind_all\n",
                "# Start TensorBoard pointing to your output directory\n",
                "# (Make sure 'gemma-3-reasoning-output' matches the 'output_dir' in your GRPOConfig!)\n",
                "# %tensorboard --logdir gemma-2-2b-reasoning-output\n",
                "\n",
                "# Define Training Arguments (GRPO)\n",
                "training_args = GRPOConfig(\n",
                "    output_dir=\"gemma-2-2b-reasoning-v2\",\n",
                "    learning_rate=5e-6,\n",
                "    per_device_train_batch_size=1,\n",
                "    per_device_eval_batch_size=2, # Reverted to 2 (must be divisible by num_generations)\n",
                "    gradient_accumulation_steps=4,\n",
                "    gradient_checkpointing=True,\n",
                "    max_prompt_length=256,\n",
                "    max_completion_length=200, # Reduced to save memory\n",
                "    num_generations=2, # Reduced to 2 to prevent OOM\n",
                "    \n",
                "    # --- STEP MATH SETTINGS ---\n",
                "    max_steps=330, \n",
                "    warmup_steps=30, # ~10% of total steps\n",
                "    \n",
                "    # --- VALIDATION SETTINGS ---\n",
                "    eval_strategy=\"steps\",      # Check validation set every X steps\n",
                "    eval_steps=10,              # Check every 50 steps\n",
                "    save_steps=10,              # Save a checkpoint every 50 steps\n",
                "    logging_steps=2,\n",
                "    \n",
                "    fp16=False,\n",
                "    bf16=True,\n",
                "    report_to=\"tensorboard\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "0f43ca0b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Starting GRPO Trainer...\n"
                    ]
                }
            ],
            "source": [
                "# 5. Initialize Trainer\n",
                "# Note: We assume you still have your 'dataset' and reward functions from before\n",
                "print(\"üöÄ Starting GRPO Trainer...\")\n",
                "trainer = GRPOTrainer(\n",
                "    model=model,\n",
                "    processing_class=tokenizer,\n",
                "    reward_funcs=[correctness_reward_func, hicra_planning_reward_func],\n",
                "    args=training_args,\n",
                "    train_dataset=dataset_train, # Your 630 questions\n",
                "    eval_dataset=dataset_test,   # Your 50 test questions <--- ADDED THIS\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "adbe9716",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='10' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [ 10/330 03:35 < 2:23:44, 0.04 it/s, Epoch 0.02/1]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mFailed to restart the Kernel. \n",
                        "\u001b[1;31mrequest to http://localhost:8888/api/kernels/28bd3f52-3caa-4829-9bcc-0a00fccc9829/restart?1766616603485 failed, reason: socket hang up. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "# Train with GRPO\n",
                "trainer.train()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "402268d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Continue training from step 60 to step 180\n",
                "trainer.args.max_steps = 180  # New target\n",
                "\n",
                "# Resume from the last checkpoint\n",
                "trainer_stats = trainer.train(resume_from_checkpoint=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2b5d57da",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Continue training from step 60 to step 180\n",
                "trainer.args.max_steps = 270  # New target\n",
                "\n",
                "# Resume from the last checkpoint\n",
                "trainer_stats = trainer.train(resume_from_checkpoint=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4ab47cb9",
            "metadata": {},
            "source": [
                "Set up the transformers inference API:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b086751",
            "metadata": {},
            "source": [
                "1. Adjusting Your Script for the Project\n",
                "Here is the adjusted script. I have updated it to fit the Gemma-9B context and added a safety step to clear memory before merging (crucial on cloud GPUs to avoid crashing at the finish line).\n",
                "\n",
                "You should append this to the end of your training notebook/script.\n",
                "\n",
                "2. Important Step for HF Spaces\n",
                "You must add your Hugging Face Token as a Secret in the Space settings, or the script won't be able to push the model.\n",
                "\n",
                "Go to your Space -> Settings.\n",
                "\n",
                "Scroll to \"Variables and secrets\".\n",
                "\n",
                "Add a New Secret: HF_TOKEN -> [Paste your Write token]."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b23e612",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import os\n",
                "import gc\n",
                "from huggingface_hub import login\n",
                "\n",
                "# --- 1. MEMORY CLEANUP (Crucial for Cloud) ---\n",
                "# RL Training fills VRAM. We need to clear it before the heavy \"Merge\" step.\n",
                "print(\"üßπ Cleaning up VRAM before merging...\")\n",
                "try:\n",
                "    del trainer\n",
                "    del batch\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "except:\n",
                "    pass\n",
                "\n",
                "# --- 2. RELOAD MODEL FOR MERGING ---\n",
                "# Sometimes it's safer to reload the base model + adapter freshly to merge\n",
                "# independent of the messy training state.\n",
                "from unsloth import FastLanguageModel\n",
                "\n",
                "print(\"üîÑ Reloading model for clean merge...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"gemma-2-9b-reasoning-v1\", # Your base model\n",
                "    max_seq_length = 4096,\n",
                "    dtype = None,\n",
                "    load_in_4bit = True,\n",
                ")\n",
                "\n",
                "# Load the adapters you just trained\n",
                "# Assuming your GRPOConfig output_dir was \"gemma-reasoning-output\"\n",
                "# and the latest checkpoint is saved there.\n",
                "from peft import PeftModel\n",
                "model = PeftModel.from_pretrained(model, \"gemma-reasoning-output/checkpoint-final\") # Update path to your actual checkpoint folder!\n",
                "\n",
                "# --- 3. LOGIN & PUSH ---\n",
                "hf_token = os.environ.get(\"HF_TOKEN\")\n",
                "if hf_token:\n",
                "    login(token=hf_token)\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No HF_TOKEN found! Check your Space 'Settings' -> 'Variables' to add it.\")\n",
                "\n",
                "repo_name = \"DataImaginations/Gemma-2-9B-Reasoning-v1\" # Repo name might be `DataImaginations/` or `david-barnes`\n",
                "\n",
                "print(f\"‚è≥ Merging to 16-bit and Pushing to: {repo_name}...\")\n",
                "\n",
                "# This takes care of the de-quantization and merging in one go\n",
                "model.push_to_hub_merged(\n",
                "    repo_name,\n",
                "    tokenizer,\n",
                "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
                "    token = hf_token\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Success! Your reasoning model is live.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ed046041",
            "metadata": {},
            "source": [
                "### 3. Configure LoRA:\n",
                "\n",
                "Unsloth handles the target modules automatically (including the tricky gate_proj, up_proj, etc. that vanilla Peft requires you to list manually)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2cde9498",
            "metadata": {},
            "source": [
                "### Check where the model is stored"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "098082b5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check where the model is cached\n",
                "from huggingface_hub import hf_hub_download\n",
                "import os\n",
                "\n",
                "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/\")\n",
                "print(f\"Model cache location: {cache_dir}\")\n",
                "print(\"\\nContents:\")\n",
                "if os.path.exists(cache_dir):\n",
                "    for item in os.listdir(cache_dir)[:10]:  # Show first 10 items\n",
                "        print(f\"  - {item}\")\n",
                "else:\n",
                "    print(\"Cache directory not found yet\")\n",
                "\n",
                "# You can also set a custom cache location if you prefer:\n",
                "# os.environ['HF_HOME'] = '/path/to/custom/cache'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ebc77db5",
            "metadata": {},
            "source": [
                "## Apply QLora"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4381ced9",
            "metadata": {},
            "source": [
                "Quick calculation:\n",
                "\n",
                "700 records\n",
                "Effective batch size = per_device_batch_size (2) √ó gradient_accumulation_steps (4) = 8\n",
                "Steps per epoch = 700 / 8 = ~88 steps\n",
                "So 60 steps = ~0.7 epochs - you haven't even completed one full pass through your data yet!\n",
                "\n",
                "Recommendations:\n",
                "\n",
                "Epochs |\tSteps |\tUse Case |\n",
                "1 |\t~90 |\tMinimum - sees all data once |\n",
                "2-3 |\t~180-270|\tSweet spot for fine-tuning |\n",
                "5+ |\t440+ |\tRisk of overfitting |\n",
                "\n",
                "Since your loss was still decreasing at step 60, you probably have room to train more. I'd suggest trying max_steps = 180 (about 2 epochs) for a good balance.\n",
                "\n",
                "Watch for:\n",
                "\n",
                "‚úÖ Good sign: Loss continues decreasing smoothly\n",
                "‚ö†Ô∏è Overfitting warning: Loss drops very low (<0.1) or starts fluctuating"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00db7b43",
            "metadata": {},
            "source": [
                "### LOGIN TO HUB\n",
                "\n",
                "When we push to HuggingFace Hub, it will merge our local QLoRa adaptor with the base model we used to train, on the hub."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "11efb6c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from huggingface_hub import login\n",
                "\n",
                "# Try to login with token from environment variable\n",
                "hf_token = os.getenv('HF_TOKEN')\n",
                "if hf_token:\n",
                "\tlogin(token=hf_token)\n",
                "\tprint(\"Logged in with HF_TOKEN environment variable\") \n",
                "else:\n",
                "\t# Skip login for local training - you can still train without pushing to hub\n",
                "\tprint(\"No HF_TOKEN found. Proceeding with local training on local GPU...\")\n",
                "\tprint(\"Note: You won't be able to push models to HuggingFace Hub without authentication\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "435fb190",
            "metadata": {},
            "source": [
                "# Push Model to hub!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "52519998",
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import os\n",
                "device = \"cuda:0\"\n",
                "\n",
                "# 1. CONFIGURATION\n",
                "# Point this to the exact folder on your disk\n",
                "checkpoint_path = \"outputs/checkpoint-180\" \n",
                "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\" # Your Hugging Face repo\n",
                "hf_token = os.getenv('HF_TOKEN')\n",
                "\n",
                "# 2. LOAD SPECIFIC CHECKPOINT\n",
                "# Unsloth is smart: if you point it to a folder, it loads the base model \n",
                "# AND applies the adapters from that folder automatically.\n",
                "print(f\"üìÇ Loading checkpoint from {checkpoint_path}...\")\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = checkpoint_path, \n",
                "    max_seq_length = 2048,\n",
                "    dtype = None,\n",
                "    load_in_4bit = True, # Keep True for fast loading (Unsloth handles the merge magic)\n",
                ")\n",
                "\n",
                "# 3. MERGE & PUSH\n",
                "# This will de-quantize the base model, merge your checkpoint-180 adapters, \n",
                "# and upload a clean 16-bit model to the Hub.\n",
                "print(f\"üöÄ Merging and pushing to {repo_name}...\")\n",
                "\n",
                "model.push_to_hub_merged(\n",
                "    repo_name,\n",
                "    tokenizer,\n",
                "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
                "    token = hf_token\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Done! Your Junior Accountant (Checkpoint 180) is live!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
