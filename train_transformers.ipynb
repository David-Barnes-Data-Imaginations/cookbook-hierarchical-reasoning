{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1f339f87",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# Force unsloth to use the local GPU memory efficiently\n",
                "os.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "fb9cc97a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Note: you may need to restart the kernel to use updated packages.\n",
                        "Collecting httpx==0.27.2\n",
                        "  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
                        "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 76 kB 12.7 MB/s eta 0:00:01\n",
                        "\u001b[?25hRequirement already satisfied: httpcore==1.* in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (1.0.9)\n",
                        "Collecting sniffio\n",
                        "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
                        "Requirement already satisfied: anyio in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (4.12.0)\n",
                        "Requirement already satisfied: idna in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (2.10)\n",
                        "Requirement already satisfied: certifi in /home/user/miniconda/lib/python3.9/site-packages (from httpx==0.27.2) (2021.5.30)\n",
                        "Requirement already satisfied: h11>=0.16 in /home/user/miniconda/lib/python3.9/site-packages (from httpcore==1.*->httpx==0.27.2) (0.16.0)\n",
                        "Requirement already satisfied: typing_extensions>=4.5 in /home/user/miniconda/lib/python3.9/site-packages (from anyio->httpx==0.27.2) (4.15.0)\n",
                        "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/user/miniconda/lib/python3.9/site-packages (from anyio->httpx==0.27.2) (1.3.1)\n",
                        "Installing collected packages: sniffio, httpx\n",
                        "  Attempting uninstall: httpx\n",
                        "    Found existing installation: httpx 0.28.1\n",
                        "    Uninstalling httpx-0.28.1:\n",
                        "      Successfully uninstalled httpx-0.28.1\n",
                        "Successfully installed httpx-0.27.2 sniffio-1.3.1\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "# run `setup_grpo_transformers.sh` first\n",
                "\n",
                "# OR Install required packages (needed after Space restarts)\n",
                "%pip install -q huggingface_hub transformers trl peft accelerate bitsandbytes datasets tensorboard\n",
                "%pip install httpx==0.27.2"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "587d407d",
            "metadata": {},
            "source": [
                "### Local login, not for use with spaces"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "014b4497",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check where the model is cached\n",
                "\"\"\"from huggingface_hub import hf_hub_download\n",
                "import os\n",
                "\n",
                "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/\")\n",
                "print(f\"Model cache location: {cache_dir}\")\n",
                "print(\"\\nContents:\")\n",
                "if os.path.exists(cache_dir):\n",
                "    for item in os.listdir(cache_dir)[:10]:  # Show first 10 items\n",
                "        print(f\"  - {item}\")\n",
                "else:\n",
                "    print(\"Cache directory not found yet\")\n",
                "\n",
                "# You can also set a custom cache location if you prefer:\n",
                "# os.environ['HF_HOME'] = '/path/to/custom/cache'\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "30c4485a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting dotenv\n",
                        "  Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
                        "Collecting python-dotenv\n",
                        "  Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
                        "Installing collected packages: python-dotenv, dotenv\n",
                        "Successfully installed dotenv-0.9.9 python-dotenv-1.2.1\n",
                        "Note: you may need to restart the kernel to use updated packages.\n",
                        "/bin/bash: uv: command not found\n"
                    ]
                }
            ],
            "source": [
                "%pip install dotenv\n",
                "!uv pip install dotenv"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "af58fc9b",
            "metadata": {},
            "source": [
                "### Server-Side HF Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "17012abc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Token set! Restart remote shell to activate.\n"
                    ]
                }
            ],
            "source": [
                "# Set remote HF_TOKEN from local .env\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "hf_token = os.getenv('HF_TOKEN')\n",
                "\n",
                "# ssh -i ~/.ssh/id_ed25519 dataimaginations-heirarchical-reasoning@ssh.hf.space \"echo 'export HF_TOKEN={hf_token}' >> ~/.bashrc\"\n",
                "print(\"‚úÖ Token set! Restart remote shell to activate.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "7a4343d1",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9b757b77057d468ab21393262b10abdc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Logged in interactively\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from huggingface_hub import login\n",
                "\n",
                "# Login using your HF token\n",
                "hf_token = os.getenv('HF_TOKEN')  # Try environment variable first\n",
                "\n",
                "if hf_token:\n",
                "    login(token=hf_token)\n",
                "    print(\"‚úÖ Logged in with HF_TOKEN environment variable\")\n",
                "else:\n",
                "    # If no env var, prompt for token (you'll need to paste it)\n",
                "    login()\n",
                "    print(\"‚úÖ Logged in interactively\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "57a44ab7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# !ssh -i ~/.ssh/id_ed25519 dataimaginations-heirarchical-reasoning@ssh.hf.space \"echo 'export HF_TOKEN=' >> ~/.bashrc\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "437aea4d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n",
                        "/tmp/ipykernel_1352/2813570285.py:4: FutureWarning: Support for Python 3.9 will be dropped in the next release (after its end-of-life on October 31, 2025). Please upgrade to Python 3.10 or newer.\n",
                        "  from trl import GRPOConfig, GRPOTrainer\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚è≥ Loading model in 4-bit...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4972202628c74da4b1c643724cb46b71",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5571775cf6dd4c10bbd1af33cdb819e8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ca2379b87c7c49429cf7e8b19a918c87",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0a55da0b864343ef9100b68d827dc2e3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "62c934ddab1249cc92eb1306e3048715",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "507e5ef12be54d8598f983f13c41333b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b3264fcf6698458c9e10d911e122d5ca",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fbea6ca4aa884687b9e6d1c79b363599",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "98c0b9ba581f4248883fa139197fa74e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "daea2ccad7e34aa9b34b130af43d1700",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6e24459a231c43e6bb4e60053f9b2102",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4ffbe629aaf144ce8b30aeadd6af84fd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "070d11707f2b4a9f9a6b85fbf7124ae6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Loaded google/gemma-2-9b-it\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "from trl import GRPOConfig, GRPOTrainer\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "MODEL_NAME = \"google/gemma-2-9b-it\"\n",
                "output_dir = \"gemma-2-9b-reasoning-v1\"\n",
                "\n",
                "print(\"‚è≥ Loading model in 4-bit...\")\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    # No attn_implementation - let it auto-select\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# Fix padding\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    model.config.pad_token_id = tokenizer.pad_token_id\n",
                "\n",
                "print(f\"‚úÖ Loaded {MODEL_NAME}\")# Changed from flash_attention_2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "1d29e3d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Prepare Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "tokenizer.padding_side = \"left\" # CRITICAL for reasoning/generation steps!\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "4cc17269",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Pad token set to: <pad>\n",
                        "‚úÖ Vocab size: 256000\n",
                        "‚úÖ Model vocab size: 256000\n"
                    ]
                }
            ],
            "source": [
                "# Fix tokenizer padding\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    model.config.pad_token_id = tokenizer.pad_token_id\n",
                "\n",
                "print(f\"‚úÖ Pad token set to: {tokenizer.pad_token}\")\n",
                "print(f\"‚úÖ Vocab size: {len(tokenizer)}\")\n",
                "\n",
                "# Gemma3 has a nested config structure (vision + text)\n",
                "if hasattr(model.config, 'text_config'):\n",
                "    print(f\"‚úÖ Model vocab size: {model.config.text_config.vocab_size}\")\n",
                "else:\n",
                "    print(f\"‚úÖ Model vocab size: {model.config.vocab_size}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "2a08ae03",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: The HICRA Logic (Strategic Grams)\n",
                "\n",
                "# These are the \"thinking words\" the paper identified. \n",
                "# When the model uses these, it is \"planning\".\n",
                "STRATEGIC_GRAMS = [\n",
                "    \"first i need to\", \"let's look at\", \"alternatively\", \"wait\", \n",
                "    \"but i'm not sure\", \"let's see if\", \"notice that\", \n",
                "    \"the final answer is\", \"let's assume\", \"we can conclude\",\n",
                "    \"implies that\", \"to solve this\", \"break it down\", \n",
                "    \"suppose that\", \"checking the\", \"recall that\"\n",
                "]\n",
                "\n",
                "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
                "    \"\"\"\n",
                "    Give reward=1.0 if completion matches answer, 0.0 otherwise\n",
                "    \"\"\"\n",
                "    rewards = []\n",
                "    for completion in completions:\n",
                "        # Extract just the number from the completion\n",
                "        # (model might generate extra text like \"The answer is 14\")\n",
                "        reward = 1.0 if str(answer).strip() in completion.strip() else 0.0\n",
                "        rewards.append(reward)\n",
                "    return rewards\n",
                "\n",
                "def hicra_planning_reward_func(prompts, completions, **kwargs):\n",
                "    \"\"\"\n",
                "    HICRA Proxy: Gives a small bonus for using 'Strategic Grams'.\n",
                "    This encourages the model to 'think' before answering.\n",
                "    \"\"\"\n",
                "    rewards = []\n",
                "    for completion in completions:\n",
                "        score = 0.0\n",
                "        # Check for presence of planning words\n",
                "        completion_lower = completion.lower()\n",
                "        for gram in STRATEGIC_GRAMS:\n",
                "            if gram in completion_lower:\n",
                "                score += 0.1 # Small bonus for EACH planning step\n",
                "        \n",
                "        # Cap the bonus so it doesn't game the system just by spamming words\n",
                "        rewards.append(min(score, 0.5)) \n",
                "    return rewards"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "eb281a58",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c0395e6e6a094a4bb9a1180ccfde1aa0",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split: 0 examples [00:00, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'prompt': 'The sum of two numbers is 45. One number is 3 more than twice the other. What is the smaller number?', 'answer': '14'}\n"
                    ]
                }
            ],
            "source": [
                "# Cell 4: Prepare Data for GRPO\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Load the file you generated with the API script\n",
                "dataset = load_dataset(\"json\", data_files=\"reasoning_dataset.json\", split=\"train\")\n",
                "\n",
                "# GRPO expects a specific format. We don't need a system prompt for simple math.\n",
                "# It just needs 'prompt' and 'answer' (which we generated).\n",
                "print(dataset[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "f1d0ce3d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sample data:\n",
                        "{'prompt': 'The sum of two numbers is 45. One number is 3 more than twice the other. What is the smaller number?', 'answer': '14'}\n",
                        "\n",
                        "Keys: dict_keys(['prompt', 'answer'])\n",
                        "\n",
                        "Prompt type: <class 'str'>\n",
                        "Answer type: <class 'str'>\n"
                    ]
                }
            ],
            "source": [
                "# Test your dataset format\n",
                "print(\"Sample data:\")\n",
                "print(dataset[0])\n",
                "print(\"\\nKeys:\", dataset[0].keys())\n",
                "print(\"\\nPrompt type:\", type(dataset[0]['prompt']))\n",
                "print(\"Answer type:\", type(dataset[0]['answer']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "2c57e8db",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîó Attaching LoRA adapters...\n"
                    ]
                }
            ],
            "source": [
                "# Attach LoRA Adapters (PEFT)\n",
                "print(\"üîó Attaching LoRA adapters...\")\n",
                "peft_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                ")\n",
                "# We wrap the model manually so GRPO knows it's a PEFT model\n",
                "model = get_peft_model(model, peft_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "3452d607",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the TensorBoard extension\n",
                "# %load_ext tensorboard\n",
                "\n",
                "# Start TensorBoard pointing to your output directory\n",
                "# (Make sure 'gemma-3-reasoning-output' matches the 'output_dir' in your GRPOConfig!)\n",
                "# %tensorboard --logdir gemma-2-reasoning-output\n",
                "\n",
                "# Define Training Arguments (GRPO)\n",
                "training_args = GRPOConfig(\n",
                "    output_dir=\"gemma-2-reasoning-output\",\n",
                "    learning_rate=5e-6, # Lower LR for RL\n",
                "    per_device_train_batch_size=1,\n",
                "    gradient_accumulation_steps=4,\n",
                "    gradient_checkpointing=True, # Critical for A10G memory\n",
                "    max_prompt_length=512,\n",
                "    max_completion_length=512, # The \"Thinking\" Space\n",
                "    num_generations=2, # Reduced from 4 to save VRAM\n",
                "    max_steps=200, \n",
                "    save_steps=50,\n",
                "    logging_steps=1,\n",
                "    fp16=False,\n",
                "    bf16=True, # Use bfloat16 on A10G\n",
                "    report_to=\"tensorboard\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "0f43ca0b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Starting GRPO Trainer...\n"
                    ]
                }
            ],
            "source": [
                "# 5. Initialize Trainer\n",
                "# Note: We assume you still have your 'dataset' and reward functions from before\n",
                "print(\"üöÄ Starting GRPO Trainer...\")\n",
                "trainer = GRPOTrainer(\n",
                "    model=model,\n",
                "    processing_class=tokenizer, # Newer TRL uses 'processing_class' instead of 'tokenizer'\n",
                "    reward_funcs=[correctness_reward_func, hicra_planning_reward_func],\n",
                "    args=training_args,\n",
                "    train_dataset=dataset,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "adbe9716",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='6' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [  6/200 03:19 < 2:41:10, 0.02 it/s, Epoch 0.03/2]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>-0.074300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>0.000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>0.000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>0.000000</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Train with GRPO\n",
                "trainer.train()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "402268d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Continue training from step 60 to step 180\n",
                "trainer.args.max_steps = 180  # New target\n",
                "\n",
                "# Resume from the last checkpoint\n",
                "trainer_stats = trainer.train(resume_from_checkpoint=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2b5d57da",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Continue training from step 60 to step 180\n",
                "trainer.args.max_steps = 270  # New target\n",
                "\n",
                "# Resume from the last checkpoint\n",
                "trainer_stats = trainer.train(resume_from_checkpoint=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4ab47cb9",
            "metadata": {},
            "source": [
                "Set up the transformers inference API:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2b086751",
            "metadata": {},
            "source": [
                "1. Adjusting Your Script for the Project\n",
                "Here is the adjusted script. I have updated it to fit the Gemma-9B context and added a safety step to clear memory before merging (crucial on cloud GPUs to avoid crashing at the finish line).\n",
                "\n",
                "You should append this to the end of your training notebook/script.\n",
                "\n",
                "2. Important Step for HF Spaces\n",
                "You must add your Hugging Face Token as a Secret in the Space settings, or the script won't be able to push the model.\n",
                "\n",
                "Go to your Space -> Settings.\n",
                "\n",
                "Scroll to \"Variables and secrets\".\n",
                "\n",
                "Add a New Secret: HF_TOKEN -> [Paste your Write token]."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b23e612",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import os\n",
                "import gc\n",
                "from huggingface_hub import login\n",
                "\n",
                "# --- 1. MEMORY CLEANUP (Crucial for Cloud) ---\n",
                "# RL Training fills VRAM. We need to clear it before the heavy \"Merge\" step.\n",
                "print(\"üßπ Cleaning up VRAM before merging...\")\n",
                "try:\n",
                "    del trainer\n",
                "    del batch\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "except:\n",
                "    pass\n",
                "\n",
                "# --- 2. RELOAD MODEL FOR MERGING ---\n",
                "# Sometimes it's safer to reload the base model + adapter freshly to merge\n",
                "# independent of the messy training state.\n",
                "from unsloth import FastLanguageModel\n",
                "\n",
                "print(\"üîÑ Reloading model for clean merge...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"gemma-2-9b-reasoning-v1\", # Your base model\n",
                "    max_seq_length = 4096,\n",
                "    dtype = None,\n",
                "    load_in_4bit = True,\n",
                ")\n",
                "\n",
                "# Load the adapters you just trained\n",
                "# Assuming your GRPOConfig output_dir was \"gemma-reasoning-output\"\n",
                "# and the latest checkpoint is saved there.\n",
                "from peft import PeftModel\n",
                "model = PeftModel.from_pretrained(model, \"gemma-reasoning-output/checkpoint-final\") # Update path to your actual checkpoint folder!\n",
                "\n",
                "# --- 3. LOGIN & PUSH ---\n",
                "hf_token = os.environ.get(\"HF_TOKEN\")\n",
                "if hf_token:\n",
                "    login(token=hf_token)\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No HF_TOKEN found! Check your Space 'Settings' -> 'Variables' to add it.\")\n",
                "\n",
                "repo_name = \"DataImaginations/Gemma-2-9B-Reasoning-v1\" # Repo name might be `DataImaginations/` or `david-barnes`\n",
                "\n",
                "print(f\"‚è≥ Merging to 16-bit and Pushing to: {repo_name}...\")\n",
                "\n",
                "# This takes care of the de-quantization and merging in one go\n",
                "model.push_to_hub_merged(\n",
                "    repo_name,\n",
                "    tokenizer,\n",
                "    save_method = \"merged_16bit\", # 16-bit is best for sharing reasoning models\n",
                "    token = hf_token\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Success! Your reasoning model is live.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ed046041",
            "metadata": {},
            "source": [
                "### 3. Configure LoRA:\n",
                "\n",
                "Unsloth handles the target modules automatically (including the tricky gate_proj, up_proj, etc. that vanilla Peft requires you to list manually)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2cde9498",
            "metadata": {},
            "source": [
                "### Check where the model is stored"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "098082b5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check where the model is cached\n",
                "from huggingface_hub import hf_hub_download\n",
                "import os\n",
                "\n",
                "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/\")\n",
                "print(f\"Model cache location: {cache_dir}\")\n",
                "print(\"\\nContents:\")\n",
                "if os.path.exists(cache_dir):\n",
                "    for item in os.listdir(cache_dir)[:10]:  # Show first 10 items\n",
                "        print(f\"  - {item}\")\n",
                "else:\n",
                "    print(\"Cache directory not found yet\")\n",
                "\n",
                "# You can also set a custom cache location if you prefer:\n",
                "# os.environ['HF_HOME'] = '/path/to/custom/cache'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ebc77db5",
            "metadata": {},
            "source": [
                "## Apply QLora"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4381ced9",
            "metadata": {},
            "source": [
                "Quick calculation:\n",
                "\n",
                "700 records\n",
                "Effective batch size = per_device_batch_size (2) √ó gradient_accumulation_steps (4) = 8\n",
                "Steps per epoch = 700 / 8 = ~88 steps\n",
                "So 60 steps = ~0.7 epochs - you haven't even completed one full pass through your data yet!\n",
                "\n",
                "Recommendations:\n",
                "\n",
                "Epochs |\tSteps |\tUse Case |\n",
                "1 |\t~90 |\tMinimum - sees all data once |\n",
                "2-3 |\t~180-270|\tSweet spot for fine-tuning |\n",
                "5+ |\t440+ |\tRisk of overfitting |\n",
                "\n",
                "Since your loss was still decreasing at step 60, you probably have room to train more. I'd suggest trying max_steps = 180 (about 2 epochs) for a good balance.\n",
                "\n",
                "Watch for:\n",
                "\n",
                "‚úÖ Good sign: Loss continues decreasing smoothly\n",
                "‚ö†Ô∏è Overfitting warning: Loss drops very low (<0.1) or starts fluctuating"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00db7b43",
            "metadata": {},
            "source": [
                "### LOGIN TO HUB\n",
                "\n",
                "When we push to HuggingFace Hub, it will merge our local QLoRa adaptor with the base model we used to train, on the hub."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "11efb6c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from huggingface_hub import login\n",
                "\n",
                "# Try to login with token from environment variable\n",
                "hf_token = os.getenv('HF_TOKEN')\n",
                "if hf_token:\n",
                "\tlogin(token=hf_token)\n",
                "\tprint(\"Logged in with HF_TOKEN environment variable\") \n",
                "else:\n",
                "\t# Skip login for local training - you can still train without pushing to hub\n",
                "\tprint(\"No HF_TOKEN found. Proceeding with local training on local GPU...\")\n",
                "\tprint(\"Note: You won't be able to push models to HuggingFace Hub without authentication\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "435fb190",
            "metadata": {},
            "source": [
                "# Push Model to hub!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "52519998",
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import os\n",
                "device = \"cuda:0\"\n",
                "\n",
                "# 1. CONFIGURATION\n",
                "# Point this to the exact folder on your disk\n",
                "checkpoint_path = \"outputs/checkpoint-180\" \n",
                "repo_name = \"DataImaginations/ministral-3B-Beancount-v1\" # Your Hugging Face repo\n",
                "hf_token = os.getenv('HF_TOKEN')\n",
                "\n",
                "# 2. LOAD SPECIFIC CHECKPOINT\n",
                "# Unsloth is smart: if you point it to a folder, it loads the base model \n",
                "# AND applies the adapters from that folder automatically.\n",
                "print(f\"üìÇ Loading checkpoint from {checkpoint_path}...\")\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = checkpoint_path, \n",
                "    max_seq_length = 2048,\n",
                "    dtype = None,\n",
                "    load_in_4bit = True, # Keep True for fast loading (Unsloth handles the merge magic)\n",
                ")\n",
                "\n",
                "# 3. MERGE & PUSH\n",
                "# This will de-quantize the base model, merge your checkpoint-180 adapters, \n",
                "# and upload a clean 16-bit model to the Hub.\n",
                "print(f\"üöÄ Merging and pushing to {repo_name}...\")\n",
                "\n",
                "model.push_to_hub_merged(\n",
                "    repo_name,\n",
                "    tokenizer,\n",
                "    save_method = \"merged_16bit\", # options: \"merged_4bit\", \"merged_16bit\"\n",
                "    token = hf_token\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Done! Your Junior Accountant (Checkpoint 180) is live!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
